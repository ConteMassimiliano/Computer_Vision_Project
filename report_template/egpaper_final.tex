\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[
natbib=true,
style=numeric,
sorting=none
]{biblatex}
\addbibresource{egbib.bib}

\usepackage{subfig}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Crowd counting on fixed camera images}

\author{Pierpaolo D'Odorico\\
{\tt\small pierpaolo.dodorico@studenti.unipd.it}
\and
Massimiliano Conte\\
{\tt\small massimiliano.conte@studenti.unipd.it}
}


\maketitle
%\thispagestyle{empty}
\begin{abstract}



In this work we compared different computer vision tecniques in order to estimate the number of people in a frame. The counting is performed on images captured from a fixed camera placed in a shopping mall. Some applications of this kind of counting on a static view are security and safety tasks, estimating the number of visitors on a mall for a/b testing purpose, planning spaces and services or verify compliance with covid-19 social distancing.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The crowd counting problem received a lot of attention in recent years, due to its direct connection with crowd control and public safety. For this reason many techniques were recently proposed.
Our idea is to compare two main tecniques in this fixed camera setting, one that is fast to implement and the other one more challenging, in order to verify if it is worth it to spend time for a more sophisticated solution. The first approach is to direcly estimate the number of people performing regression with a deep convolutional neural network, such as the VGG16 network \cite{simonyan2014very}. We chose this very deep network since it is easy to handle for our purposes, unlike more complex architectures that have, for example, skip layer connections, and also because it is the base for the second technique. The second approach performs an undirect estimate of the number of people. First it is estimated the density of people in the image, then starting from the obtained density map the count is inferred. This second approach represents the base idea for the state of the art methods in croud counting, where images could have a completely different number of people on  differents enviroment and perspective. After implementing the two approaches on our problem, we found that a simple regression based on neural networks could perform as good as density based approach, probably exploiting the fixed background and because density estimation methods are well suited for dense datasets.


%------------------------------------------------------------------------
\section{Related work}

\subsection{VGG net}
We based our work on information contained in different papers about computer vision tasks and crowd counting. The first one is related to the base network of both approaches, the VGG16 net \cite{simonyan2014very}. In this paper the authors investigated the effect of a deeper (with respect to previous architectures) convolutional neural network on the classification accuracy in the
large-scale image recognition setting, specifically on the \textit{imageNet} dataset \cite{deng2009imagenet}. In this convolutional neural networks they used very small (3x3) convolution filters, which have shown a significant improvement on the prior state of the art configurations. They also pushed the depth to 16-19 weight layers (Fig.\ref{fig:vgg16}). Those convolutional filters learned during \textit{imageNet} classification task can be useful in our application, giving a meaningful feature extraction for finding people in images.

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{pics/vgg16.png}
  \caption{VGG16 deep CNN architecture.}
  \label{fig:vgg16}
\end{figure}

\subsection{Density based approach}
The crowd counting based on density map estimation is a well known approach. One solution that is robust with respect to a variety of image properties is the \textit{Multi-Column Convolutional Neural Network (MCCNN)} \cite{zhang2016single}. The authors designed a system capable of detecting heads of different sizes, both in dense or sparse situations. Their work is first based on the generation of ground-truth density maps via geometry-adaptive kernels, for handling both dense and sparse images. Their architecture is designed in such a way that is able to detect heads of different sizes, in particular they built a neural network with three branches, one for head size, small, medium and large heads. For the training they pre trained separately each branch, and then the full network. The output of the network was designed and trained to be the estimation of the ground truth density maps. In order to train such a network, since are required ground truth density maps, a datatset with head annotations is a requisite, and for this reason they also introduced \textit{Shanghai-tech}, a new large scale crowd counting dataset. 

\begin{figure}[h!]
	\includegraphics[width=\linewidth]{pics/MCCNN.png}
	\caption{MCCNN architecture.}
	\label{fig:MCCNN}
\end{figure}


 The models that we implemented are related to a more recent paper that simplifies the convolutional neural network of the previous work, by removing the multy-branch structure and going further with the deepness. They also experimented with the stride of convolutions, in an architecture they called \textit{CSRNet} \cite{li2018csrnet}. Designing a model which is automatically able to distinguish heads of different sizes achieved better results with respect to \textit{MCCNN}. This architecture is basically the \textit{VGG16} net (without the classifier), with other convolutional layers on top of it. For the training they fine tuned the network, starting from \textit{ImageNet} learned weights for the $VGG16$ part and random normal initialization for the others. Since those methods \cite{zhang2016single,li2018csrnet} don't use dense layers, they can work with images of any size.

\begin{figure}[h!]
	\includegraphics[width=0.5\columnwidth]{pics/CSRnet.png}
	\centering
	\caption{Best CSRnet  (kernel size - \# of filters - stride).}
	\centering
	\label{fig:CSRnet}
\end{figure}


%------------------------------------------------------------------------

\section{Datasets}
\subsection{Mall dataset}
\subsection{Shanghai tech dataset}
This dataset was introduced in the \textit{MCCNN} paper \cite{zhang2016single}. This is a large-scale crowd counting dataset  which contains 1198 annotated images, with
a total of 330,165 people with centers of their heads annotated. It consists of two parts: there are 482 images in Part A which are randomly crawled from the Internet, and 716 images in Part B which are taken from the busy streets of metropolitan areas in Shanghai. The crowd density varies significantly between the two subsets, in particular the part A is extremely dense, while part B is sparser, but still a dense dataset. Both Part A and Part B are divided into training and testing: 300
images of Part A are used for training and the remaining 182 images for testing, and 400 images of Part B are for training and 316 for testing. We used Part B because is similar to the \textit{mall dataset}, since it is sparser.


\begin{figure}[h!]%
	\centering
	\subfloat[\centering An image from Part A.]{{\includegraphics[width=0.4\linewidth]{pics/ShanghaiA.jpg} }}%
	\qquad
	\subfloat[\centering An image from Part B.]{{\includegraphics[width=0.4\linewidth]{pics/ShanghaiB.jpg} }}%
	\label{fig:Shanghai}%
\end{figure}

\section{Method}
\subsection{Regression based approach}
\subsection{Density based approach}
\subsubsection{Ground truth generation}
Since we want the model to estimate the crowd density, we need ground truth density maps in order to perform supervised learning. A density map is a single channel image with positive values associated to each pixel. The generation of this image involve the head annotations: for each labeled image we have the coordinates of each head in that image. We can see each annotation as a sparse matrix, whose dimensions are the same as the image, with all zeros but one entry equals to one, corresponding to the pixel in the center of the head. As it is done in \cite{zhang2016single}, we used geometry-adaptive kernels. The density map $F$ is obtained via the following formula: \begin{equation}
	F(x) = \sum_{i = 1}^{N}{\delta(x-x_i) * G_{\sigma_i}(x)} \text{ with } \sigma_i = \beta \bar{d^i}
\end{equation}
Where:
\begin{itemize}
	\item $x$ represent a given pixel coordinates;
	\item $N$ is the number of annotated heads;
	\item $x_i$ is the $i$-th annotated head pixel coordinates;
	\item $\delta(\cdot)$ is the discrete version of the delta function: this function is zero on all possible points but in zero. In our case $\delta(0) = 1$ (in general we would have $\delta(0) = +\infty$ s.t. $\int_{Domain(x)}\delta(x)d(x) = 1$);
	\item $*$ is the convolution operator;
	\item $G_{\sigma}$ is the 2-d gaussian filter;
	\item $\bar{d^i}$  is the mean euclidean distance between $x_i$ and $k$ neightbors.
\end{itemize}   
As suggested in \cite{zhang2016single}, we used $\beta = 0.3$.

\begin{figure}[h!]
	\includegraphics[width=\linewidth]{pics/DensityGT.png}
	\caption{An example of ground truth density estimation.}
	\label{fig:DensityGT}
\end{figure}
\subsubsection{Overall procedure}
Our goal is to use this model on the \textit{Mall dataset}, but we don't have annotation for those images. So we used \textit{Shanghai-Tech} Part B dataset in order to pretrain the model. The first step was to build the architecture as described in \cite{li2018csrnet}, in particular the one that performed the best in their experiments (Figure \ref{fig:CSRnet}). We froze all the \textit{VGG16} layers, with \textit{ImageNet} pretrained weights, and train the other layers using SGD as described in \cite{li2018csrnet}. Then we fine tuned the whole neural network by train it again having unfrozen all the parameters, so we let the \textit{VGG16} layers to adapt their filters in order to catch features that are more relevant for the crowd counting task. We checked the performance of the network by making predictions and evaluatng them using \textit{Mean Absolut Error}.
\begin{equation}
	\hat{y}^{(i)} = \sum_{x \in X^{(i)}}\hat{F}^{(i)}(x)
\end{equation}
Basically the prediction is the sum over each pixel of the estimated density map.
\begin{equation}
	MAE = \frac{1}{m}  \sum_{i = 1}^{m} |\hat{y}^{(i)} - y^{(i)}|
\end{equation}
Since \textit{VGG16} has 3 max pooling layers (Figure \ref{fig:vgg16}), we built the ground truth density maps such that its dimension would be $1/8$ of the original images, in order to make the predictions match the ground truth values.


\begin{figure}[h!]
	\includegraphics[width=\linewidth]{pics/DMpred.png}
	\caption{Test images, GT and CRSnet estimated density}
	\label{fig:DMpred}
\end{figure}

As we can see from Figure \ref{fig:DMpred}, the estimation of the density maps provided by the trained \textit{CSRnet} model seem reasonable.
\\
Once we had the \textit{CSRnet} trained on \textit{Shanghai-Tech} Part B dataset we adapt this model to the \textit{Mall dataset} in order to let it exploit the static background and the fixed perspective. The first thing we have done is to fine tune the whole network on the \textit{Mall dataset}. After this, since the results were worst than the simpler approach, we decided to use the fine tuned \textit{CSRnet} as feature extractor and to build on top of it a regressor. So we froze all the \textit{CSRnet} weights and added some dense layers (we experimented different configurations that will be explained in the next section), and that we trained the regressor using adam. We could add dense layers since images on \textit{Mall dataset} have fixed size. 
\section{Experiments}


\printbibliography

%{\small
%\bibliographystyle{ieee_fullname}
%\bibliography{egbib}
%}

\end{document}
